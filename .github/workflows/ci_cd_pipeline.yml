name: Python ML CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build_and_test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11"] # Test with multiple Python versions

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip' # Cache pip dependencies for faster builds

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Linting with Flake8
        run: |
          pip install flake8
          flake8 src/ tests/ main.py --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 src/ tests/ main.py --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Run Pytest Unit Tests
        run: |
          pytest tests/

  train_and_evaluate:
    needs: build_and_test # This job runs only after build_and_test is successful
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # Use a specific Python version for training

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ML Training and Evaluation
        run: |
          python main.py
        env:
          # Example of passing environment variables, if your main.py needed them
          # ML_MODEL_OUTPUT_DIR: 'models'

      - name: Upload Trained Model (Artifact)
        uses: actions/upload-artifact@v4
        with:
          name: trained-model-${{ github.sha }} # Unique name for each run
          path: models/iris_classifier.pkl # Path to your saved model
          retention-days: 5 # Keep artifact for 5 days

      - name: Display Evaluation Metrics (Optional: for reporting)
        run: |
          # You might parse metrics from main.py output or a saved file here
          # For simplicity, we assume main.py prints them
          echo "Model training and evaluation completed. Check logs above for metrics."

  deploy:
    needs: train_and_evaluate # This job runs only after training and evaluation are successful
    runs-on: ubuntu-latest
    # Define an environment for deployment. This can be used for protection rules (e.g., manual approval).
    # environment:
    #   name: production
    #   url: https://your-model-api.com # Replace with the actual URL of your deployed model/service
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies for deployment
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt # Installs fastapi, uvicorn, etc.
          # Also install the `requests` library to test the API endpoint
          pip install requests

      - name: Download Trained Model Artifact
        uses: actions/download-artifact@v4
        with:
          name: trained-model-${{ github.sha }}
          path: models/

      - name: Start Local Model Deployment Server
        run: |
          # The `&` runs the process in the background
          # We specify the `src.api:app` to point uvicorn to our FastAPI application
          # `PYTHONPATH` is set to ensure the API can find modules in `src/`
          uvicorn src.api:app --host 0.0.0.0 --port 8000 &
          echo "API server started in the background on port 8000."
        env:
          PYTHONPATH: "${{ github.workspace }}" # Add project root to Python path
          # MODELS_DIR: "${{ github.workspace }}/models" # This env var is not used by src/api.py directly

      - name: Wait for the server to start
        # Give the server a few seconds to boot up before we try to test it
        run: sleep 5

      - name: Test the Deployed API Endpoint
        run: |
          echo "Testing the deployed API..."
          # Use `curl` to make a POST request to the /predict endpoint
          # The data should match the expected input format of your model
          curl --fail -X POST http://localhost:8000/predict \
          -H "Content-Type: application/json" \
          -d '{ "data": [[5.1, 3.5, 1.4, 0.2], [6.0, 2.7, 5.1, 1.6]] }'
          
          echo "API test successful!"